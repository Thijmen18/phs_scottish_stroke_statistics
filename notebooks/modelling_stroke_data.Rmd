---
title: "R Notebook"
output: html_notebook
---

# PHS Scottish Stroke Statistics

This notebook contains steps to create a predictive multivariate model to predict 
future stroke incidence rates using future population predictions per year
as published by NRS (National Records Scotland)
 
```{r}
library(tidyverse)
library(GGally)
library(janitor)
library(modelr)
library(broom)
library(ggfortify)

activity_hb <- read_csv("../clean_data/stroke_activity_healthboard.csv")
```

## Joining

Next step: join the population estimates to the activity_hb dataset.

```{r}
pop_est <- read_csv("../clean_data/pop_est_nhs_format.csv")
```
```{r}
# in activity_hb change financial year into regular year
# since its mid-year population estimates, this does not matter for the join

activity_hb_reduced <- activity_hb %>% 
  mutate(year = as.numeric(str_sub(financial_year, start = 1, end = 4), .before = financial_year)) %>% 
  select(year, hbr, admission_type, age_group, sex, diagnosis, number_of_discharges, crude_rate, easr)
```

```{r}
# Healthboard numbers in population estimate dataset are mixed of old and new numbers
# recode so all are new codes

pop_est_hb <- pop_est %>% 
  mutate(hbr = recode(hbr,
                     "S08000001" = "S08000015",
                     "S08000002" = "S08000016",
                     "S08000003" = "S08000017",
                     "S08000004" = "S08000029",
                     "S08000005" = "S08000019",
                     "S08000006" = "S08000020",
                     "S08000007" = "S08000031",
                     "S08000008" = "S08000022",
                     "S08000009" = "S08000032",
                     "S08000010" = "S08000024",
                     "S08000011" = "S08000025",
                     "S08000012" = "S08000026",
                     "S08000013" = "S08000030",
                     "S08000014" = "S08000028"))
```

Join!
```{r}
# join
activity_popest_hb <- activity_hb_reduced %>% 
  left_join(pop_est_hb, by = c("hbr" = "hbr", "sex" = "sex", "year" = "year", "age_group" = "age_group")) %>% 
  select(!"...1")

activity_popest_hb

# save the join to clean_data
write.csv(activity_popest_hb, file = "../clean_data/activity_popest_hb.csv")

#write.csv(population_estimates_nhs_format, file = "clean_data/pop_est_nhs_format.csv")
```

## further data wrangling

```{r}
#check for NAs, and drop the missing number of discharges missing 
# (these are removed by NHS due to confidentiality reasons)
# there are also missing population_size numbers since, 
# not all age_groups exist in my dataset

activity_popest_clean <- activity_popest_hb %>% 
  drop_na("number_of_discharges") %>% 
  drop_na("population_size") 
```

Secondly, there are multiple diagnosis types and admission types that are subdivided 
in multiple categories. I am only interested in the total in admission_type this is
categorised under "all" and for diagnosis this is categorised under Cerebrovascular disease

```{r}
activity_popest_clean <- activity_popest_clean %>% 
  filter(admission_type == "All") %>% 
  filter(diagnosis == "Cerebrovascular Disease")

activity_popest_clean %>% 
  distinct(diagnosis)
```

## Step 1: look at relationships between variables and Crude_rate/ number_of_discharges

```{r}
# How is crude_rate distributed?

activity_popest_clean %>% 
  ggplot() +
  aes(x = year, y = crude_rate/100000) +
  geom_point()

# so it runs from 0 to almost 6%
```

```{r message = FALSE}
# first check the relationships between variables and the crude_rate 

set_1 <- activity_popest_clean %>% 
  select(crude_rate, year, hbr, age_group, sex, number_of_discharges)

set_2 <- activity_popest_clean %>% 
  select(crude_rate, admission_type, diagnosis, easr, population_size)

ggpairs(set_1)
ggpairs(set_2)
# variables of interest: hbr, age_group and sex (but not so obvious from the ggpairs plot)?
# definitely interaction between those!
```

## Step 2: convert character columns to factor

```{r}
# change crude_rate into proportion (/1000)
activity_popest_red <- activity_popest_clean %>% 
  mutate(crude_prop = crude_rate/100000, .before = hbr) %>% 
# select only columns of interest
  select(year, crude_prop, hbr, age_group, sex, number_of_discharges, 
         crude_rate, easr, area_name, population_size) %>% 
  mutate(year = as.factor(year),
         hbr = as.factor(hbr),
         sex = as.factor(sex),
         area_name = as.factor(area_name),
         age_group = as.factor(age_group))
  
```

## step 3: divide into test and train dataset

```{r}
n_data <- nrow(activity_popest_red)


test_index <- sample(1:n_data, size = n_data*0.2)

test_data <- slice(activity_popest_red, test_index)
train_data <- slice(activity_popest_red, -test_index)


# checking proportions in all datasets:
activity_popest_red %>% 
  janitor::tabyl(hbr)

train_data %>% 
  janitor::tabyl(sex)

test_data %>% 
  janitor::tabyl(sex)

# not all that far off, we are good to go.
```

## step 4: modelling

```{r}
# model with single predictor (age_group)
model1 <- glm(crude_prop ~ age_group, data = train_data, family = binomial(link = 'logit'))

# model with single pred (sex)
model2 <- glm(crude_prop ~ sex, data = train_data, family = binomial(link = 'logit'))

#model with single pred (hbr)
model3 <- glm(crude_prop ~ hbr, data = train_data, family = binomial(link = 'logit'))

#model with single pred (population)
model4 <- glm(crude_prop ~ population_size, data = train_data, family = binomial(link = 'logit'))

clean_names(tidy(model1))
clean_names(tidy(model2))
clean_names(tidy(model3))
clean_names(tidy(model4))

```

We see that all predictors (of interest) show to be non-significant in terms of 
predicting proportion of strokes (when added in a single
predictor model). This basically tells us that our data might be too aggregated and
we have too little variables that can explain/predict stroke proportions in a population.

Multiple options:
- in an ideal situation I have each row representing a single person, with additional 
variables (e.g. smoking, diet, obesity level, etc. etc.) that can help me predict 
the chance of getting a stroke.
- Augmentation: find more data online about the smoking level under different 
age-groups in each health board. Obesity levels, etc. This helps to add additional 
variables for each group to explain stroke proportions.
- alternative is to do future forecasting (see classnotes), however: this data is not
necessarily seasonal and more importantly the data is on a yearly scale. That means 
I only have effectively 9 datapoints.
  - Ãnd I have future population predictions I want to use to predict strokes.
  
## trial with building linear regression model
  
-> I can decide to continue with just creating a simple linear regression model to 
explore the data and explore possibilities.


```{r}
# linear model
model1A <- lm(number_of_discharges ~ age_group, data = train_data)

model2A <- lm(number_of_discharges ~ sex, data = train_data)

model3A <- lm(number_of_discharges ~ hbr, data = train_data)

model4A <- lm(number_of_discharges ~ population_size, data = train_data)

summary(model1A)
summary(model2A)
```

This code-chunk shows that each predictor is significant (according to Jamie this 
is highly unreliable as the logistic regression model above with proportions already
showed its not). 
Basically the linear regression model of single predictors now says: with an increasing
population size, the number of strokes increase. (and similar for the other variables).

## Let's continue and create a linear regression using automated model development

This is not the right way -> please see above under "Multiple options:"
but this allows me (for the sake of the story and steps) do predictions on stroke incidence
proportions for the future based on future population estimates. And create spatial heatmaps

```{r}
library(leaps)
library(glmulti)
```

I follow an automated approach to create a predictive linear model using Leaps with
an exhaustive search. 

```{r}
regsubsets_exhaustive <- regsubsets(number_of_discharges ~ year + hbr + age_group + sex + population_size,
                                    data = train_data, nvmax = 5, method = "exhaustive")

plot(regsubsets_exhaustive)

```
From the plot, it seems like the best perfoming model has:
hbr (certain categories), age_group and population size. 
Not sex and not years..

```{r}
# see whats in the model
sum_regsubsets_exhaustive <- summary(regsubsets_exhaustive)

sum_regsubsets_exhaustive$bic
```

```{r}
# plotting the BIC scores

plot(summary(regsubsets_exhaustive)$bic, type = "b")
```

So the BIC score is lowest at 5 different variables. These are:

```{r}
sum_regsubsets_exhaustive$which[5, ]
```
-> hbr, age_group, population_size

## let's create the model as found above

```{r}
model_linear <- lm(number_of_discharges ~ hbr + age_group + population_size, data = train_data)
  
summary(model_linear)
# Adjusted R-squared:  0.763

##quick test if adding sex, would help in explaining more of the variance
model_linear_a <- lm(number_of_discharges ~ hbr + age_group + population_size + sex, data = train_data)

summary(model_linear_a)  
# adjusted R-squared: 0.764, so no reason to include

## quick test if interaction between hbr and age_group is helping here
model_linear_b <- lm(number_of_discharges ~ hbr + age_group + population_size + age_group:hbr, data = train_data)
summary(model_linear_b)  

# The adjusted R-squared is now really high: 0.95, but I am afraid I am now really 
# overfitting. And since we are just creating a model for the sake of predicting values. I 
# will stick to model with hbr + age_group + population_size as the predictors.
```

```{r}
# summary of model we choose:

summary(model_linear)
autoplot(model_linear)
```
As expected, diagnostic plots look awful. But we are continuing for the sake of 
'the story'

## Fit the model on both training and test dataset and check mean squared error

```{r}
#training set

predictions_train <- train_data %>% 
  add_predictions(model_linear) %>% 
  select(number_of_discharges, pred)

predictions_test <- test_data %>% 
  add_predictions(model_linear) %>% 
  select(number_of_discharges, pred)

# calculate the mean squared error

mse_train <- mean((predictions_train$pred - train_data$number_of_discharges)^2)
mse_test <- mean((predictions_test$pred - test_data$number_of_discharges)**2)

mse_train
mse_test

# so the error is lower when predicting on the training data. But error is 
# not even that bad (does not seem to be random) when applied to the test data set.
```

```{r}
train_data %>% 
  add_predictions(model_linear) %>% 
  view()
```

EEEEHHHHH, this is not helpful. Some of my predicted models are even negative.
So I conclude this dataset is not useful to model stroke incidences, and make a predicter model.
It would be helpful to e.g. augment the data -> get more helpful variables included to
help explain stroke incidences. 
E.g. smoking rate, obesity rate per health board and age_group.

So for now, I stop the modelling and decide that if I want to calculate future 
stroke incidence levels I will just calculate this based on current (most recent years proportions)

```{r}
train_data %>% 
  distinct(hbr)
```

```{r}
activity_popest_red %>% 
  head()
```
```{r}
train_data %>% 
  head()
```




